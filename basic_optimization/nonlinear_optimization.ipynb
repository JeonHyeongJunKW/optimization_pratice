{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비선형 최적화(nonlinear optimization)란?\n",
    "\n",
    "비선형으로 되어있는 함수 f(x)를 최소화 또는 최적화하는 파라미터 x를 찾는 것.\n",
    "\n",
    "### 호칭\n",
    "- f(x)의 값은 일반적으로 1차원의 실수(R^1)로 표현되며, 목적함수(objective function) 또는 비용함수(cost function)이라고 불린다.\n",
    "- 모든 x에 대해서 ```f(x') <= f(x)```를 만족하는 x'를 찾는 일이다. 보통 x'를 전역 최소자(global minimizer)라고 부른다.\n",
    "\n",
    "### 방법론\n",
    "- 최소자를 찾는 방법으로 가장 많이 사용하는 방법은 처음 시점 x0를 정하고, 그 다음 스텝에서 f(x1) <= f(x0)를 만족하는 x1을 찾는 방법\n",
    "  이다. 이 방법들은 현재 있는 점에서 하강 방향을 결정하고 어느 정도 내려갈지를 선택해서 그 다음 점을 결정한다.\n",
    "  다시 같은 반복적인 방법(iterative method)으로 우리가 원하는 조건에 맞을 때까지 위의 일을 수행한다.\n",
    "- 방법의 핵심은 먼저 하강 방향(descent direction)을 정하고, 내려가는 크기 (step size)를 정해서 내려가는 것이다.\n",
    "\n",
    "### 주의할 점.\n",
    "- 이러한 구조는 목적함수의 구조를 모르고 시행하는 방법이다. (convex함수 인지는 모름) 따라서 국소 최소자를 찾는 방법이다.\n",
    "- 초기값을 어떻게 취하느냐에 따라서 다른 결과가 나올 수도 있다. 왜냐하면 목적함수가 항상 convex 함수는 아닐수도 있기 때문이다.(convex라면 반드시 전역최소자로 수렴가능)\n",
    "\n",
    "\n",
    "### 종료조건\n",
    "\n",
    ": 위의 반복은 다음과 같은 종료조건에 따라 멈추고, 최소자를 구한다.\n",
    "\n",
    "1. 반복간에 파라미터의 변화가 너무 적은 경우\n",
    "2. 반복간에 목적함수의 값의 변화가 너무 적은 경우\n",
    "3. 반복간에 gradient(또는 자코비안의 gradient 중에서 최대값)의 크기가 너무 작은 경우\n",
    "4. 목적함수에 매우 0에 가까운경우(최소값이라는 뜻)\n",
    "5. 반복횟수가 일정 이상일 경우\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACPUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
