{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비선형 최적화(nonlinear optimization)란?\n",
    "\n",
    "비선형으로 되어있는 함수 f(x)를 최소화 또는 최적화하는 파라미터 x를 찾는 것.\n",
    "\n",
    "### 호칭\n",
    "- f(x)의 값은 일반적으로 1차원의 실수(R^1)로 표현되며, 목적함수(objective function) 또는 비용함수(cost function)이라고 불린다.\n",
    "- 모든 x에 대해서 ```f(x') <= f(x)```를 만족하는 x'를 찾는 일이다. 보통 x'를 전역 최소자(global minimizer)라고 부른다.\n",
    "\n",
    "### 방법론\n",
    "- 최소자를 찾는 방법으로 가장 많이 사용하는 방법은 처음 시점 x0를 정하고, 그 다음 스텝에서 f(x1) <= f(x0)를 만족하는 x1을 찾는 방법\n",
    "  이다. 이 방법들은 현재 있는 점에서 하강 방향을 결정하고 어느 정도 내려갈지를 선택해서 그 다음 점을 결정한다.\n",
    "  다시 같은 반복적인 방법(iterative method)으로 우리가 원하는 조건에 맞을 때까지 위의 일을 수행한다.\n",
    "- 방법의 핵심은 먼저 하강 방향(descent direction)을 정하고, 내려가는 크기 (step size)를 정해서 내려가는 것이다.\n",
    "\n",
    "### 주의할 점.\n",
    "- 이러한 구조는 목적함수의 구조를 모르고 시행하는 방법이다. (convex함수 인지는 모름) 따라서 국소 최소자를 찾는 방법이다.\n",
    "- 초기값을 어떻게 취하느냐에 따라서 다른 결과가 나올 수도 있다. 왜냐하면 목적함수가 항상 convex 함수는 아닐수도 있기 때문이다.(convex라면 반드시 전역최소자로 수렴가능)\n",
    "\n",
    "\n",
    "### 종료조건\n",
    "\n",
    ": 위의 반복은 다음과 같은 종료조건에 따라 멈추고, 최소자를 구한다.\n",
    "\n",
    "1. 반복간에 파라미터의 변화가 너무 적은 경우\n",
    "2. 반복간에 목적함수의 값의 변화가 너무 적은 경우\n",
    "3. 반복간에 gradient(또는 자코비안의 원소 중에서 최대값)의 크기가 너무 작은 경우\n",
    "4. 목적함수에 매우 0에 가까운경우(최소값이라는 뜻)\n",
    "5. 반복횟수가 일정 이상일 경우\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "### 설명과 유도\n",
    "- 최소자 x'를 초깃값 x0부터 시작해서 순차적으로 x1, x2,..를 찾아내는 방법\n",
    "- 단위벡터 u에 대해서, q0(t) = x0 + t * u 라 하면 q0(t)는 x0를 지나고 방향이 u인 직선을 의미한다.\n",
    "  phi0(t) = f(q0(t)) = f(x0 + t * u) 라고 하면, phi0(t) : R -> R인 함수가 된다. 이때, x0가 x1으로 바뀔때,\n",
    "  f(x)값도 감소해야 하므로 phi0(t)의 미분은 0보다 작아야한다! (감소함수가 되어야하니까.)\n",
    "- phi0(t)의 미분 = ▽f(x0) (내적) u = |▽f(x0)| * cos(theta)\n",
    "  ※ theta는 미분의 방향과 파라미터의 변화 방향사이의 각도를 나타냄.\n",
    "- 따라서 u = - ▽f(x0) / |▽f(x0)|가 phi0(t)를 낮추는 미분방향이 된다.\n",
    "- 이제 x1의 값은 x0 - t * ▽f(x0) 중에서 f(x_0 - t * ▽f(x0)) 의 값이 최소가 되는 t를 구하면된다.\n",
    "\n",
    "\n",
    "### 예제 코드 결론\n",
    "- step size에 따라서 오히려 발산하는 경우도 존재."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36120557]\n",
      " [0.61154324]] , cost :  0.3862768255838136\n",
      "[[0.31685841]\n",
      " [0.51140817]] , cost :  0.27649772404043965\n",
      "[[0.26793495]\n",
      " [0.43358827]] , cost :  0.19846031838818246\n",
      "[[0.2270223 ]\n",
      " [0.36732694]] , cost :  0.14244903288102667\n",
      "[[0.19233524]\n",
      " [0.31120508]] , cost :  0.10224576716491812\n",
      "[[0.16294908]\n",
      " [0.26365714]] , cost :  0.07338903390613143\n",
      "[[0.13805267]\n",
      " [0.22337392]] , cost :  0.05267651118495073\n",
      "[[0.1169601 ]\n",
      " [0.18924542]] , cost :  0.03780966559891473\n",
      "[[0.09909019]\n",
      " [0.16033129]] , cost :  0.027138676813322687\n",
      "[[0.08395056]\n",
      " [0.13583485]] , cost :  0.019479351840634065\n",
      "[[0.07112405]\n",
      " [0.11508113]] , cost :  0.013981711442355216\n",
      "[[0.06025726]\n",
      " [0.0974983 ]] , cost :  0.010035665275550738\n",
      "[[0.05105077]\n",
      " [0.08260189]] , cost :  0.007203308260088763\n",
      "[[0.04325091]\n",
      " [0.06998144]] , cost :  0.005170324882823029\n",
      "[[0.03664276]\n",
      " [0.05928923]] , cost :  0.003711108622416439\n",
      "[[0.03104424]\n",
      " [0.05023064]] , cost :  0.0026637256883273192\n",
      "[[0.0263011 ]\n",
      " [0.04255608]] , cost :  0.0019119447217998036\n",
      "[[0.02228265]\n",
      " [0.03605409]] , cost :  0.001372338238594535\n",
      "[[0.01887817]\n",
      " [0.03054552]] , cost :  0.0009850244202331852\n",
      "[[0.01599384]\n",
      " [0.02587858]] , cost :  0.0007070218413861421\n",
      "[[0.0135502 ]\n",
      " [0.02192468]] , cost :  0.0005074796867256491\n",
      "[[0.01147991]\n",
      " [0.01857489]] , cost :  0.00036425413949624957\n",
      "[[0.00972594]\n",
      " [0.0157369 ]] , cost :  0.0002614510129385386\n",
      "[[0.00823995]\n",
      " [0.01333251]] , cost :  0.0001876619226925538\n",
      "[[0.00698099]\n",
      " [0.01129549]] , cost :  0.00013469826271793708\n",
      "[[0.00591439]\n",
      " [0.00956969]] , cost :  9.668249008060664e-05\n",
      "[[0.00501075]\n",
      " [0.00810757]] , cost :  6.939587563768806e-05\n",
      "[[0.00424518]\n",
      " [0.00686884]] , cost :  4.9810338475006445e-05\n",
      "[[0.00359657]\n",
      " [0.00581938]] , cost :  3.5752410300984316e-05\n",
      "[[0.00304707]\n",
      " [0.00493026]] , cost :  2.566203887514871e-05\n",
      "[[0.00258152]\n",
      " [0.00417698]] , cost :  1.8419464133625496e-05\n",
      "[[0.0021871 ]\n",
      " [0.00353879]] , cost :  1.3220954914010118e-05\n",
      "[[0.00185294]\n",
      " [0.00299812]] , cost :  9.489616395473485e-06\n",
      "[[0.00156983]\n",
      " [0.00254004]] , cost :  6.811370276878491e-06\n",
      "[[0.00132998]\n",
      " [0.00215196]] , cost :  4.889003213119757e-06\n",
      "[[0.00112678]\n",
      " [0.00182317]] , cost :  3.50918412100322e-06\n",
      "[[0.00095462]\n",
      " [0.00154461]] , cost :  2.518790161981326e-06\n",
      "[[0.00080877]\n",
      " [0.00130862]] , cost :  1.8079142220329502e-06\n",
      "[[0.0006852 ]\n",
      " [0.00110868]] , cost :  1.2976681756045534e-06\n",
      "[[0.00058051]\n",
      " [0.00093929]] , cost :  9.314284236800255e-07\n",
      "[[0.00049182]\n",
      " [0.00079578]] , cost :  6.685521959686514e-07\n",
      "[[0.00041667]\n",
      " [0.00067419]] , cost :  4.798672956195409e-07\n",
      "[[0.00035301]\n",
      " [0.00057119]] , cost :  3.444347693325196e-07\n",
      "[[0.00029908]\n",
      " [0.00048392]] , cost :  2.4722524624642273e-07\n",
      "[[0.00025338]\n",
      " [0.00040998]] , cost :  1.7745108166649237e-07\n",
      "[[0.00021467]\n",
      " [0.00034734]] , cost :  1.2736921840588027e-07\n",
      "[[0.00018187]\n",
      " [0.00029427]] , cost :  9.142191552156736e-08\n",
      "[[0.00015408]\n",
      " [0.00024931]] , cost :  6.56199884260791e-08\n",
      "[[0.00013054]\n",
      " [0.00021122]] , cost :  4.710011660194243e-08\n",
      "[[0.0001106 ]\n",
      " [0.00017895]] , cost :  3.380709197191683e-08\n",
      "[[9.36986093e-05]\n",
      " [1.51607535e-04]] , cost :  2.426574603321701e-08\n",
      "[[7.93827357e-05]\n",
      " [1.28443964e-04]] , cost :  1.7417245796761176e-08\n",
      "[[6.72541329e-05]\n",
      " [1.08819473e-04]] , cost :  1.2501591780014903e-08\n",
      "[[5.69786158e-05]\n",
      " [9.21933370e-05]] , cost :  8.973278488335913e-09\n",
      "[[4.82730579e-05]\n",
      " [7.81074485e-05]] , cost :  6.440757964753834e-09\n",
      "[[4.08975910e-05]\n",
      " [6.61736923e-05]] , cost :  4.62298848904142e-09\n",
      "[[3.46489951e-05]\n",
      " [5.60632518e-05]] , cost :  3.318246499366218e-09\n",
      "[[2.93550997e-05]\n",
      " [4.74975491e-05]] , cost :  2.3817406979612094e-09\n",
      "[[2.48700396e-05]\n",
      " [4.02405693e-05]] , cost :  1.709544108133085e-09\n",
      "[[2.10702357e-05]\n",
      " [3.40923574e-05]] , cost :  1.2270609727390832e-09\n",
      "[[1.78509901e-05]\n",
      " [2.88835087e-05]] , cost :  8.80748629799127e-10\n",
      "[[1.51236015e-05]\n",
      " [2.44705013e-05]] , cost :  6.321757158989889e-10\n",
      "[[1.28129208e-05]\n",
      " [2.07317414e-05]] , cost :  4.5375731763959354e-10\n",
      "[[1.08552807e-05]\n",
      " [1.75642131e-05]] , cost :  3.2569378755506894e-10\n",
      "[[9.19674140e-06]\n",
      " [1.48806402e-05]] , cost :  2.337735153314262e-10\n",
      "[[7.79160435e-06]\n",
      " [1.26070807e-05]] , cost :  1.6779582097853876e-10\n",
      "[[6.60115314e-06]\n",
      " [1.06808901e-05]] , cost :  1.2043895348001762e-10\n",
      "[[5.59258668e-06]\n",
      " [9.04899534e-06]] , cost :  8.644757319204702e-11\n",
      "[[4.73811547e-06]\n",
      " [7.66643188e-06]] , cost :  6.204955037270581e-11\n",
      "[[4.01419584e-06]\n",
      " [6.49510531e-06]] , cost :  4.4537360151240924e-11\n",
      "[[3.40088129e-06]\n",
      " [5.50274153e-06]] , cost :  3.196762002829713e-11\n",
      "[[2.88127287e-06]\n",
      " [4.66199743e-06]] , cost :  2.294542664413194e-11\n",
      "[[2.44105355e-06]\n",
      " [3.94970761e-06]] , cost :  1.6469558991729722e-11\n",
      "[[2.06809375e-06]\n",
      " [3.34624598e-06]] , cost :  1.182136979141479e-11\n",
      "[[1.75211714e-06]\n",
      " [2.83498509e-06]] , cost :  8.48503495543188e-12\n",
      "[[1.48441747e-06]\n",
      " [2.40183791e-06]] , cost :  6.0903109762446914e-12\n",
      "[[1.25761866e-06]\n",
      " [2.03486973e-06]] , cost :  4.371447846967487e-12\n",
      "[[1.06547163e-06]\n",
      " [1.72396930e-06]] , cost :  3.1376979522545994e-12\n",
      "[[9.02682046e-07]\n",
      " [1.46057023e-06]] , cost :  2.2521482090681634e-12\n",
      "[[7.64764502e-07]\n",
      " [1.23741496e-06]] , cost :  1.6165263938054699e-12\n",
      "[[6.47918884e-07]\n",
      " [1.04835478e-06]] , cost :  1.1602955664054287e-12\n",
      "[[5.48925687e-07]\n",
      " [8.88180419e-07]] , cost :  8.3282636558182e-13\n",
      "[[4.65057305e-07]\n",
      " [7.52478526e-07]] , cost :  5.97778510312662e-13\n",
      "[[3.94002871e-07]\n",
      " [6.37510038e-07]] , cost :  4.290680052401856e-13\n",
      "[[3.33804589e-07]\n",
      " [5.40107171e-07]] , cost :  3.0797251815643333e-13\n",
      "[[2.82803786e-07]\n",
      " [4.57586138e-07]] , cost :  2.2105370426424775e-13\n",
      "[[2.39595213e-07]\n",
      " [3.87673198e-07]] , cost :  1.586659110412081e-13\n",
      "[[2.02988322e-07]\n",
      " [3.28442004e-07]] , cost :  1.1388577002284703e-13\n",
      "[[1.71974466e-07]\n",
      " [2.78260531e-07]] , cost :  8.174388895878391e-14\n",
      "[[1.45699105e-07]\n",
      " [2.35746105e-07]] , cost :  5.867338281828787e-14\n",
      "[[1.23438263e-07]\n",
      " [1.99727305e-07]] , cost :  4.211404540683322e-14\n",
      "[[1.04578575e-07]\n",
      " [1.69211688e-07]] , cost :  3.022823528041065e-14\n",
      "[[8.86003902e-08]\n",
      " [1.43358443e-07]] , cost :  2.169694692924473e-14\n",
      "[[7.50634551e-08]\n",
      " [1.21455222e-07]] , cost :  1.557343661260755e-14\n",
      "[[6.35947797e-08]\n",
      " [1.02898515e-07]] , cost :  1.1178159245990647e-14\n",
      "[[5.3878362e-08]\n",
      " [8.7177021e-08]] , cost :  8.023357158533093e-15\n",
      "[[4.56464808e-08]\n",
      " [7.38575574e-08]] , cost :  5.758932099350241e-15\n",
      "[[3.86723191e-08]\n",
      " [6.25731267e-08]] , cost :  4.133593740078521e-15\n",
      "[[3.27637145e-08]\n",
      " [5.30128037e-08]] , cost :  2.9669732014975747e-15\n",
      "[[2.77578644e-08]\n",
      " [4.49131680e-08]] , cost :  2.1296069550941285e-15\n"
     ]
    }
   ],
   "source": [
    "# 예제 1\n",
    "import numpy as np\n",
    "\n",
    "## f(x, y) = 4 * x^2 - 4 * x * y + 2 * y^2\n",
    "init_xy = np.random.rand(2, 1)\n",
    "iterations = 100\n",
    "def f(xy):\n",
    "    return 4 * xy[0, 0] ** 2 - 4 * xy[0, 0] * xy[1, 0] + 2 * xy[1, 0] ** 2\n",
    "\n",
    "def gradient_f(xy):\n",
    "    return np.array([[8*xy[0, 0] - 4 * xy[1, 0]], [-4 * xy[0, 0] + 4 * xy[1, 0]]])\n",
    "\n",
    "updated_xy = init_xy\n",
    "step_size = 0.1 # 최소로 만드는 t 대신에 작은 t를 사용함. 1을 사용할 경우 발산하는 문제가 있음. step size도 중요함.\n",
    "for iter in range(iterations):\n",
    "    updated_xy = updated_xy - gradient_f(updated_xy) * step_size\n",
    "    print(updated_xy,\", cost : \", f(updated_xy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton method\n",
    "\n",
    "### 설명과 유도\n",
    "- 일반적인 테일러 급수의 2차 까지 표현방법은 아래와 같이 h가 작다면 근사적으로 성립한다.  \n",
    "  f(x + h) = f(x) + ▽f(x) * h + 0.5 * h^T * Hf(x) * h  \n",
    "  ※ 여기서 hf(x)는 f(x)에 대한 헤세 행렬  \n",
    "\n",
    "- 고정된 x에 대해서 위의 식은 다음과 같이 h에 대한 식으로 나타낼 수 있다.  \n",
    "  L(h) = f(x) + ▽f(x)h + 0.5 * h^T Hf(x) h  \n",
    "  즉, L(h)의 최소값이 되는 최소자 h를 구하는 것이 x에서 x + h로 이동하는 단계가 된다. \n",
    "- L'(h) = ▽f(x) + Hf(x) * h = 0 에서, Hf(x)가 양의 확정 행렬이면( = h^T Hf(x) h > 0), L'(h) = 0을 만족시키는  \n",
    "   h_n = - Hf(x)^-1 * ▽f(x)가 파라미터 x의 변화량이다. \n",
    "\n",
    "### 예제 결과\n",
    "- 바로 수렴한다.\n",
    "- f가 미분가능 한 함수라고하면, 초깃값 x_0이 충분히 국소 최소자에 가깝고 Hf(x)가 양 확정 행렬이면 x*에 수렴한다.\n",
    "- 모든 함수에 대해서 해당 방법이 좋은 결과를 보여주는 건 아님. (ex, 예제 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n",
      "[[0.]\n",
      " [0.]] , cost :  0.0\n"
     ]
    }
   ],
   "source": [
    "# 예제 2\n",
    "import numpy as np\n",
    "\n",
    "# 위의 예제를 다시 사용한다.\n",
    "init_xy = np.random.rand(2, 1)\n",
    "iterations = 4\n",
    "def f(xy):\n",
    "    return 4 * xy[0, 0] ** 2 - 4 * xy[0, 0] * xy[1, 0] + 2 * xy[1, 0] ** 2\n",
    "\n",
    "def gradient_f(xy):\n",
    "    return np.array([[8*xy[0, 0] - 4 * xy[1, 0]], [-4 * xy[0, 0] + 4 * xy[1, 0]]])\n",
    "\n",
    "def Hessian_f(xy):\n",
    "    return np.array([[8, -4],[-4, 4]])\n",
    "\n",
    "updated_xy = init_xy\n",
    "step_size = 0.1 # 최소로 만드는 t 대신에 작은 t를 사용함. 1을 사용할 경우 발산하는 문제가 있음. step size도 중요함.\n",
    "for iter in range(iterations):\n",
    "    h = -1.0 * np.linalg.inv(Hessian_f(updated_xy)) @ gradient_f(updated_xy)\n",
    "    updated_xy = updated_xy + h\n",
    "    print(updated_xy,\", cost : \", f(updated_xy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.16666667]\n",
      " [-3.53574359]] , cost :  3.8262112494167413\n",
      "[[ 1.56928105]\n",
      " [13.95095909]] , cost :  19.83108176801894\n",
      "[[  -2.60418153]\n",
      " [-279.34406653]] , cost :  434.0793231520877\n",
      "[[5.29556906e+00]\n",
      " [1.22016999e+05]] , cost :  191677.53864348424\n"
     ]
    }
   ],
   "source": [
    "# 예제 3\n",
    "import numpy as np\n",
    "\n",
    "# 위의 예제를 다시 사용한다.\n",
    "#init_xy = np.random.rand(2, 1)\n",
    "init_xy = np.array([[1],[2]])\n",
    "iterations = 4\n",
    "\n",
    "def f(xy):\n",
    "    return 0.5 * (xy[0, 0] ** 2) *(xy[0, 0] / 6.0 + 1.0) + xy[1, 0] * np.arctan(xy[1, 0]) - 0.5 * np.log(xy[1, 0]**2 + 1)\n",
    "\n",
    "def gradient_f(xy):\n",
    "    return np.array([[xy[0, 0]**3/0.3 + xy[0, 0]], [np.arctan(xy[1, 0])]])\n",
    "\n",
    "def Hessian_f(xy):\n",
    "    return np.array([[xy[0, 0]**2 + 1, 0],[0, 1/(xy[1, 0]**2 + 1)]])\n",
    "\n",
    "updated_xy = init_xy\n",
    "for iter in range(iterations):\n",
    "    h = -1.0 * np.linalg.inv(Hessian_f(updated_xy)) @ gradient_f(updated_xy)\n",
    "    updated_xy = updated_xy + h\n",
    "    print(updated_xy,\", cost : \", f(updated_xy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenberg-Marquardt Type Damped Newton Method\n",
    "\n",
    "### 설명과 유도\n",
    "- Gradient descent 탐색 방법과 Newton 탐색 방법의 혼합형이다.\n",
    "- 최소자로부터 상대적으로 먼 거리 점에서 시작할 때는 Gradient descent 탐색방법을 이용하고 최소자 금방에서는 수렴속도가 좋은 newton 탐색 방법을 사용한다.\n",
    "- Hf(x)가 양의 확정행렬이 아닌 경우를 대신해서 Hf(x) + mu * I 를 사용한다. mu는 Hf(x)와 같은 크기의 단위 행렬이다.  \n",
    "  L = Hf(x) + mu * I라고 하자.\n",
    "- v를 Hf(x)의 고유벡터라고 하면, Hf(x) * v = lambda * v를 만족한다.  \n",
    "  Lv = (Hf(x) + mu * I)v = lambda * v + mu * v = (lambda + mu) * v 이므로, v가 행렬 L의 고유벡터가 된다.\n",
    "- mu를 적당히 크게 잡으면 행렬 L의 모든 고유치가 양수이므로 L은 양의 확정행렬이 된다.\n",
    "- 그래서 h를 구하는 대신에 다음 식을 만족하는 h_dn을 구한다.  \n",
    "  ▽f(x) + (Hf(x) + mu * I) * h_dn = 0\n",
    "- mu가 작으면, 기존 newton method랑 동일해지고, mu가 상당히 크다면, h_dn = -▽f(x)/mu 가 된다.\n",
    "\n",
    "### mu를 어떻게 업데이트 해야하는가?\n",
    "- 이득 비율(gain ratio)를 계산해서, 매번 mu를 업데이트 해야한다.\n",
    "- p = (f(x) - f(x + h_dn))/(q(0) - q(h_dn)), where q(h) = f(x) + ▽f(x)^T * h + 0.5 * h^T Hf(x) h\n",
    "- 위의 식에서 p값이 1에 가깝게 나오면, mu를 줄이고 반대인 경우는 mu값을 크게 한다.\n",
    "- p > 0.75, mu = mu/3\n",
    "- p < 0.25, mu = mu*2\n",
    "- 추가로 p가 0.75나 0.25 근처 값에서 불연속적으로 변하는 피하기 위해서, 다음과 같이 수정한다.\n",
    "- if p > 0 then mu = mu * max{1/3, 1-(2 * p - 1)^3} else mu = mu*2\n",
    "\n",
    "### 정지조건은 어떻게 설정하는가?\n",
    "- ||▽f(x)||_{inf} <= epsilon_1 또는 ||h_dn|| <= epsilon_2(epsilon_2 + ||x||) 사용한다.\n",
    "- epsilon_1 = 10^-8, epsilon_2 = 10^-12를 많이 사용한다.\n",
    "\n",
    "### 예제 결과\n",
    "- 잘 수렴하는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44444444]\n",
      " [ 1.07737607]] , cost :  0.5925039747221679\n",
      "[[-0.07468274]\n",
      " [ 0.42380845]] , cost :  0.0900488431471055\n",
      "[[-0.02857521]\n",
      " [ 0.15513904]] , cost :  0.0123925735235305\n",
      "[[-0.00500374]\n",
      " [ 0.02593798]] , cost :  0.0003488599498334437\n",
      "[[-0.00033399]\n",
      " [ 0.00172302]] , cost :  1.540177024649969e-06\n",
      "[[-7.78412139e-06]\n",
      " [ 4.01541936e-05]] , cost :  8.364758178764e-10\n",
      "[[-6.14276254e-08]\n",
      " [ 3.16872817e-07]] , cost :  5.2112978285270624e-14\n",
      "[[-1.62437925e-10]\n",
      " [ 8.37931837e-10]] , cost :  7.153228026075986e-19\n",
      "수렴완료\n"
     ]
    }
   ],
   "source": [
    "# 예제 4\n",
    "import numpy as np\n",
    "\n",
    "# 위의 예제를 다시 사용한다.\n",
    "#init_xy = np.random.rand(2, 1)\n",
    "init_xy = np.array([[1],[2]])\n",
    "iterations = 10000\n",
    "init_mu = 1\n",
    "I = np.eye(2, 2)\n",
    "epsilon = 1.0/10.0**8\n",
    "\n",
    "def f(xy):\n",
    "    return 0.5 * (xy[0, 0] ** 2) *(xy[0, 0] / 6.0 + 1.0) + xy[1, 0] * np.arctan(xy[1, 0]) - 0.5 * np.log(xy[1, 0]**2 + 1)\n",
    "\n",
    "def gradient_f(xy):\n",
    "    return np.array([[xy[0, 0]**3/0.3 + xy[0, 0]], [np.arctan(xy[1, 0])]])\n",
    "\n",
    "def Hessian_f(xy):\n",
    "    return np.array([[xy[0, 0]**2 + 1, 0],[0, 1/(xy[1, 0]**2 + 1)]])\n",
    "\n",
    "def q(xy, h):\n",
    "    return (f(xy) + gradient_f(xy).T @ h + 0.5 * h.T @ Hessian_f(xy) @ h)[0, 0]\n",
    "\n",
    "updated_xy = init_xy\n",
    "mu = init_mu\n",
    "for iter in range(iterations):\n",
    "    h_dn = -1.0 * np.linalg.inv(Hessian_f(updated_xy) + mu * I) @ gradient_f(updated_xy)\n",
    "    p = (f(updated_xy) - f(updated_xy + h_dn)) / (q(updated_xy, np.zeros((2, 1))) - q(updated_xy, h_dn))\n",
    "    updated_xy = updated_xy + h_dn\n",
    "    if p > 0:\n",
    "        mu = mu/3.0 if mu/3.0 > 1-(2*p -1)**2 else 1-(2*p -1)**2\n",
    "    else :\n",
    "        mu = mu  * 2\n",
    "    print(updated_xy,\", cost : \", f(updated_xy))\n",
    "    if np.linalg.norm(gradient_f(updated_xy), np.inf) <= epsilon:\n",
    "        print(\"수렴완료\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACPUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
