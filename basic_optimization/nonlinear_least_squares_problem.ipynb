{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비선형 최소 자승법 (nonlinear least squares problem)\n",
    "\n",
    "### nonlinear optimization과의 관계\n",
    "- 이전에 비선형 최적화 문제들이 주어진 함수 f : R^n -> R가 x_0에서부터 시작하여,  \n",
    "  f(x_0) > f(x_1) > f(x_2) ...  \n",
    "  를 만족하는 파라미터 x를 찾아가는 방법이었다.\n",
    "- 이제 다루고자 하는 문제는 f : R^n -> R^m을 만족하는 다변수 함수에 대해서  \n",
    "  f(x_0) > f(x_1) > f(x_2) ...  -> q = (q1, q2,..., qm)^T  \n",
    "  를 만족하는 파라미터를 찾는 문제이다.\n",
    "- f(x) = q를 만족하는 x를 만족하는 문제는 f_hat(x) = f(x) - q = 0을 만족하는 문제로 생각할 수 있다.  \n",
    "  이 문제는 ||f_hat(x)|| = 0에 가까워지는 문제라고 볼 수 있으며, 이 문제는 미분식의 간단함을 위해서  0.5 ||f_hat(x)||^2라는  \n",
    "  문제로 변형되어 사용된다.\n",
    "- 다시 정리하면 아래와 같다.  \n",
    "  F(x) = 0.5 ∑^m (f_i(x))^2 = 0.5 ||f(x)||^2 = 0.5 f(x)^T*f(x) 이고  \n",
    "  F(x)가 최소인 x*의 값을 구하는 방법으로 비선형 최소자승법이라고 한다.  \n",
    "  ※ 여기서 m은 보통 샘플의 수를 나타낸다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Newton 탐색방법\n",
    "\n",
    "### 특징\n",
    "- Levenberg marquardt 탐색 방법의 기본방법이고, 충분히 작은 ||h||에 대하여  \n",
    "  l(h) = f(x + h) = f(x) + J(x)h 라는 근사식에서 시작한다.\n",
    "\n",
    "### 유도\n",
    "- F(x + h)의 근사식은 다음과 같다.  \n",
    "  F(x + h) = L(h) = 0.5 l(h)^T l(h)\n",
    "  = 0.5 * f^t(x) * f(x) + h^t * J^t(x) * f(x) + 0.5 * h^t * J^t(x) * J(x) * h  \n",
    "  = F(x) + h^t * J^t(x) * f(x) + 0.5 * h^t * J^t(x) * J(x) * h\n",
    "- L(h)가 최소가 되는 h를 찾으면,  \n",
    "  L'(h) = J^t(x) * f(x) + J^t(x) * J(x) * h  \n",
    "  L''(h) = J^t(x) * J(x) 이고\n",
    "- 최소값을 가지는 L'(h_gn) = 0에서  \n",
    "  J^t(x) * J(x) * h_gn = - J^t(x) * f(x)  \n",
    "  이다.\n",
    "- 만약 J(x)의 열벡터가 일차 독립이면, L''(h) = J^t(x)J(x)는 양의 확정행렬이고, 전역 최소값이 존재한다.\n",
    "- 여기서 구해진 h_gn 은 하강 방향벡터이자. L(h)의 유일한 전역 최소자이다.\n",
    "\n",
    "### 업데이트 방법\n",
    "1. J^t(x) * J(x) * h_gn = - J^t(x) * f(x)에서 h_gn을 구한다.\n",
    "2. x_{k+1} = x_k + a*h_gn 로 업데이트한다.  \n",
    "   ※ a는 step size로 직선탐색 알고리즘을 이용한다.\n",
    "\n",
    "### 예제 결과\n",
    "1. 예제 1과 같이 F(x, y)가 컨벡스 함수가 아니라면 잘 수렴하지 못한다. 이것은 Levenberg maquardt 탐색방법도 비슷하다.\n",
    "2. 예제 2와 같이 F(x, y)가 컨벡스 함수면 빠르게 수렴함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 예제 1 가상의 a,b를 추론해보자. \n",
    "## y = a * cos(b*x) + b sin (a*x)\n",
    "\n",
    "sample_size = 100\n",
    "sample_x = 2* np.pi * np.random.rand(sample_size, 1)\n",
    "noise = 0#0.01 * np.random.rand(sample_size, 1)\n",
    "\n",
    "a = 100\n",
    "b = 102\n",
    "\n",
    "sample_y = a * np.cos( b * sample_x) + b * np.sin(a * sample_x)\n",
    "noised_sample_y = sample_y + noise\n",
    "\n",
    "iterations = 1000\n",
    "init_a = 90\n",
    "init_b = 102.6\n",
    "step_size = 1\n",
    "def f(x, arg_a, arg_b):\n",
    "    return arg_a * np.cos( arg_b * x) + arg_b * np.sin(arg_a * x)\n",
    "\n",
    "def gradient_f(x, arg_a, arg_b):\n",
    "    gradient_a = np.cos( arg_b * x) + arg_b * np.cos(arg_a * x) * x\n",
    "    gradient_b = -1.0 * arg_a * np.sin( arg_b * x) * x + np.sin(arg_a * x)\n",
    "    return np.hstack((gradient_a, gradient_b))\n",
    "\n",
    "def F(x, y, arg_a, arg_b):\n",
    "    residual = y - f(x, arg_a, arg_b)\n",
    "    return (0.5 * residual.T * residual)[0, 0]\n",
    "\n",
    "updated_a = init_a\n",
    "updated_b = init_b\n",
    "for iter in range(iterations):\n",
    "    jacobian = gradient_f(sample_x, updated_a, updated_b)\n",
    "    residual = noised_sample_y - f(sample_x, updated_a, updated_b)\n",
    "    j_t_j = jacobian.T @ jacobian\n",
    "    minus_j_t_f = jacobian.T @ residual\n",
    "    cost = F(sample_x, noised_sample_y, updated_a, updated_b)\n",
    "    if iter % 100 ==0:\n",
    "        print(\"Iter : \",iter,\", Cost : \", cost, \", Parameter a : \", updated_a, \", b : \", updated_b)\n",
    "    a_b_change = np.linalg.solve(j_t_j, minus_j_t_f)\n",
    "    updated_a += step_size * a_b_change[0, 0]\n",
    "    updated_b += step_size * a_b_change[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 예제 2 가상의 a, b를 추론해보자.\n",
    "## f(x) = a * x^2 + b\n",
    "\n",
    "sample_size = 100\n",
    "sample_x = np.random.rand(sample_size, 1)\n",
    "noise = 0.1 * np.random.rand(sample_size, 1)\n",
    "\n",
    "a = 100\n",
    "b = 102\n",
    "\n",
    "sample_y = a * sample_x * sample_x + b * np.ones((sample_size, 1))\n",
    "noised_sample_y = sample_y + noise\n",
    "\n",
    "iterations = 3\n",
    "init_a = 90\n",
    "init_b = 102.6\n",
    "step_size = 1\n",
    "def f(x, arg_a, arg_b):\n",
    "    return arg_a * x * x + arg_b * np.ones(x.shape)\n",
    "\n",
    "def gradient_f(x, arg_a, arg_b):\n",
    "    gradient_a = x * x\n",
    "    gradient_b = np.ones(x.shape)\n",
    "    return np.hstack((gradient_a, gradient_b))\n",
    "\n",
    "def F(x, y, arg_a, arg_b):\n",
    "    residual = y - f(x, arg_a, arg_b)\n",
    "    return (0.5 * residual.T * residual)[0, 0]\n",
    "\n",
    "updated_a = init_a\n",
    "updated_b = init_b\n",
    "for iter in range(iterations):\n",
    "    jacobian = gradient_f(sample_x, updated_a, updated_b)\n",
    "    residual = noised_sample_y - f(sample_x, updated_a, updated_b)\n",
    "    j_t_j = jacobian.T @ jacobian\n",
    "    minus_j_t_f = jacobian.T @ residual\n",
    "    cost = F(sample_x, noised_sample_y, updated_a, updated_b)\n",
    "    if iter % 1 ==0:\n",
    "        print(\"Iter : \",iter,\", Cost : \", cost, \", Parameter a : \", updated_a, \", b : \", updated_b)\n",
    "    a_b_change = np.linalg.solve(j_t_j, minus_j_t_f)\n",
    "    updated_a += step_size * a_b_change[0, 0]\n",
    "    updated_b += step_size * a_b_change[1, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenberg-Marquardt\n",
    "\n",
    "### 설명과 유도\n",
    "- Gradient descent 탐색 방법과 Newton 탐색 방법의 혼합형이다.\n",
    "- 최소자로부터 상대적으로 먼 거리 점에서 시작할 때는 Gradient descent 탐색방법을 이용하고 최소자 금방에서는 수렴속도가 좋은 newton 탐색 방법을 사용한다.\n",
    "- 다음과 같이 damping 인자를 추가한다.  \n",
    "  (J^t(x) * J(x) + mu * I) * h_lm = - J^t(x) * f(x)\n",
    "- mu가 작으면, 기존 newton method랑 동일해지고, mu가 상당히 크다면, h_lm = -J^t(x) * f(x)/mu 가 된다.\n",
    "\n",
    "### mu를 어떻게 업데이트 해야하는가?\n",
    "- 이득 비율(gain ratio)를 계산해서, 매번 mu를 업데이트 해야한다.\n",
    "- p = (F(x) - F(x + h_dn))/(L(0) - L(h_dn)), where L(h) = F(x) + h^t * J^t(x) * f(x) + 0.5 * h^t * J^t(x) * J(x) * h\n",
    "- 위의 식에서 p값이 1에 가깝게 나오면, mu를 줄이고 반대인 경우는 mu값을 크게 한다.\n",
    "- p > 0.75, mu = mu/3\n",
    "- p < 0.25, mu = mu*2\n",
    "- 추가로 p가 0.75나 0.25 근처 값에서 불연속적으로 변하는 피하기 위해서, 다음과 같이 수정한다.\n",
    "- if p > 0 then mu = mu * max{1/3, 1-(2 * p - 1)^3} else mu = mu*2\n",
    "\n",
    "### 정지조건은 어떻게 설정하는가?\n",
    "- ||▽f(x)||_{inf} <= epsilon_1 또는 ||h_dn|| <= epsilon_2(epsilon_2 + ||x||) 사용한다.\n",
    "- epsilon_1 = 10^-8, epsilon_2 = 10^-8를 많이 사용한다.\n",
    "\n",
    "### 예제 결과\n",
    "- 예제 3도 마찬가지로 잘 수렴하지 못하는 것을 확인할 수 있다.\n",
    "- 예제 4도 마찬가지로 잘 수렴한다.\n",
    "- 이러한 결과를 봤을 때, 여전히 convex하지 못한 문제의 경우 수렴이 잘못한다는것을 알았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 예제 3 가상의 a,b를 추론해보자.(예제 1과 같음)\n",
    "## y = a * cos(b*x) + b sin (a*x)\n",
    "\n",
    "sample_size = 100\n",
    "sample_x = 2* np.pi * np.random.rand(sample_size, 1)\n",
    "noise = 0#0.01 * np.random.rand(sample_size, 1)\n",
    "\n",
    "a = 100\n",
    "b = 102\n",
    "\n",
    "sample_y = a * np.cos( b * sample_x) + b * np.sin(a * sample_x)\n",
    "noised_sample_y = sample_y + noise\n",
    "\n",
    "iterations = 1000\n",
    "init_a = 90\n",
    "init_b = 102.6\n",
    "init_mu = 1\n",
    "epsilon = 1.0/10.0**8\n",
    "step_size = 1\n",
    "def f(x, arg_a, arg_b):\n",
    "    return arg_a * np.cos( arg_b * x) + arg_b * np.sin(arg_a * x)\n",
    "\n",
    "def gradient_f(x, arg_a, arg_b):\n",
    "    gradient_a = np.cos( arg_b * x) + arg_b * np.cos(arg_a * x) * x\n",
    "    gradient_b = -1.0 * arg_a * np.sin( arg_b * x) * x + np.sin(arg_a * x)\n",
    "    return np.hstack((gradient_a, gradient_b))\n",
    "\n",
    "def F(x, y, arg_a, arg_b):\n",
    "    residual = y - f(x, arg_a, arg_b)\n",
    "    return (0.5 * residual.T * residual)[0, 0]\n",
    "\n",
    "def L(x, y, arg_a, arg_b, h):\n",
    "    first_term = F(x, y, arg_a, arg_b)\n",
    "    second_term = (h.T @ gradient_f(x, arg_a, arg_b).T @ f(x, arg_a, arg_b))[0, 0]\n",
    "    third_term = (0.5 * h.T @ gradient_f(x, arg_a, arg_b).T @ gradient_f(x, arg_a, arg_b) @ h)[0, 0]\n",
    "    return first_term + second_term + third_term\n",
    "\n",
    "updated_a = init_a\n",
    "updated_b = init_b\n",
    "mu = init_mu\n",
    "for iter in range(iterations):\n",
    "    jacobian = gradient_f(sample_x, updated_a, updated_b)\n",
    "    residual = noised_sample_y - f(sample_x, updated_a, updated_b)\n",
    "    j_t_j_plus_mu = jacobian.T @ jacobian + mu * np.eye(2)\n",
    "    minus_j_t_f = jacobian.T @ residual\n",
    "    cost = F(sample_x, noised_sample_y, updated_a, updated_b)\n",
    "    a_b_change = np.linalg.solve(j_t_j_plus_mu, minus_j_t_f)\n",
    "    p = (F(sample_x, sample_y, updated_a, updated_b) -\\\n",
    "        F(sample_x, sample_y, updated_a + step_size * a_b_change[0, 0], updated_b + step_size * a_b_change[0, 0])) / \\\n",
    "        (L(sample_x, sample_y, updated_a, updated_b, np.zeros((2, 1))) -\\\n",
    "        L(sample_x, sample_y, updated_a, updated_b, a_b_change))\n",
    "    updated_a += step_size * a_b_change[0, 0]\n",
    "    updated_b += step_size * a_b_change[1, 0]\n",
    "    if p > 0:\n",
    "        mu = mu/3.0 if mu/3.0 > 1-(2*p -1)**2 else 1-(2*p -1)**2\n",
    "    else :\n",
    "        mu = mu  * 2\n",
    "    if iter % 1 ==0:\n",
    "        print(\"Iter : \",iter,\", Cost : \", cost, \", Parameter a : \", updated_a, \", b : \", updated_b)\n",
    "    if np.linalg.norm(jacobian, np.inf) <= epsilon:\n",
    "        print(\"수렴완료\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter :  0 , Cost :  6137.483760095832 , Parameter a :  96.63028750944837 , b :  102.58611410907068\n",
      "Iter :  1 , Cost :  1.4175470734315632 , Parameter a :  99.35081660208111 , b :  102.29826647686185\n",
      "Iter :  2 , Cost :  0.023559890147092186 , Parameter a :  99.94623491626972 , b :  102.07090376388517\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 예제 4 가상의 a,b를 추론해보자.(예제 2와 같음)\n",
    "## f(x) = a * x^2 + b\n",
    "\n",
    "sample_size = 100\n",
    "sample_x = np.random.rand(sample_size, 1)\n",
    "noise = 0.1 * np.random.rand(sample_size, 1)\n",
    "\n",
    "a = 100\n",
    "b = 102\n",
    "\n",
    "sample_y = a * sample_x * sample_x + b * np.ones((sample_size, 1))\n",
    "noised_sample_y = sample_y + noise\n",
    "\n",
    "iterations = 3\n",
    "init_a = 40\n",
    "init_b = 30\n",
    "init_mu = 1\n",
    "epsilon = 1.0/10.0**8\n",
    "step_size = 1\n",
    "def f(x, arg_a, arg_b):\n",
    "    return arg_a * x * x + arg_b * np.ones(x.shape)\n",
    "\n",
    "def gradient_f(x, arg_a, arg_b):\n",
    "    gradient_a = x * x\n",
    "    gradient_b = np.ones(x.shape)\n",
    "    return np.hstack((gradient_a, gradient_b))\n",
    "\n",
    "def F(x, y, arg_a, arg_b):\n",
    "    residual = y - f(x, arg_a, arg_b)\n",
    "    return (0.5 * residual.T * residual)[0, 0]\n",
    "\n",
    "def L(x, y, arg_a, arg_b, h):\n",
    "    first_term = F(x, y, arg_a, arg_b)\n",
    "    second_term = (h.T @ gradient_f(x, arg_a, arg_b).T @ f(x, arg_a, arg_b))[0, 0]\n",
    "    third_term = (0.5 * h.T @ gradient_f(x, arg_a, arg_b).T @ gradient_f(x, arg_a, arg_b) @ h)[0, 0]\n",
    "    return first_term + second_term + third_term\n",
    "\n",
    "updated_a = init_a\n",
    "updated_b = init_b\n",
    "mu = init_mu\n",
    "for iter in range(iterations):\n",
    "    jacobian = gradient_f(sample_x, updated_a, updated_b)\n",
    "    residual = noised_sample_y - f(sample_x, updated_a, updated_b)\n",
    "    j_t_j_plus_mu = jacobian.T @ jacobian + mu * np.eye(2)\n",
    "    minus_j_t_f = jacobian.T @ residual\n",
    "    cost = F(sample_x, noised_sample_y, updated_a, updated_b)\n",
    "    a_b_change = np.linalg.solve(j_t_j_plus_mu, minus_j_t_f)\n",
    "    p = (F(sample_x, sample_y, updated_a, updated_b) -\\\n",
    "        F(sample_x, sample_y, updated_a + step_size * a_b_change[0, 0], updated_b + step_size * a_b_change[0, 0])) / \\\n",
    "        (L(sample_x, sample_y, updated_a, updated_b, np.zeros((2, 1))) -\\\n",
    "        L(sample_x, sample_y, updated_a, updated_b, a_b_change))\n",
    "    updated_a += step_size * a_b_change[0, 0]\n",
    "    updated_b += step_size * a_b_change[1, 0]\n",
    "    if p > 0:\n",
    "        mu = mu/3.0 if mu/3.0 > 1-(2*p -1)**2 else 1-(2*p -1)**2\n",
    "    else :\n",
    "        mu = mu  * 2\n",
    "    if iter % 1 ==0:\n",
    "        print(\"Iter : \",iter,\", Cost : \", cost, \", Parameter a : \", updated_a, \", b : \", updated_b)\n",
    "    if np.linalg.norm(jacobian, np.inf) <= epsilon:\n",
    "        print(\"수렴완료\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACPUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
