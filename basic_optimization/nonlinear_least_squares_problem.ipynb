{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비선형 최소 자승법 (nonlinear least squares problem)\n",
    "\n",
    "### nonlinear optimization과의 관계\n",
    "- 이전에 비선형 최적화 문제들이 주어진 함수 f : R^n -> R가 x_0에서부터 시작하여,  \n",
    "  f(x_0) > f(x_1) > f(x_2) ...  \n",
    "  를 만족하는 파라미터 x를 찾아가는 방법이었다.\n",
    "- 이제 다루고자 하는 문제는 f : R^n -> R^m을 만족하는 다변수 함수에 대해서  \n",
    "  f(x_0) > f(x_1) > f(x_2) ...  -> q = (q1, q2,..., qm)^T  \n",
    "  를 만족하는 파라미터를 찾는 문제이다.\n",
    "- f(x) = q를 만족하는 x를 만족하는 문제는 f_hat(x) = f(x) - q = 0을 만족하는 문제로 생각할 수 있다.  \n",
    "  이 문제는 ||f_hat(x)|| = 0에 가까워지는 문제라고 볼 수 있으며, 이 문제는 미분식의 간단함을 위해서  0.5 ||f_hat(x)||^2라는  \n",
    "  문제로 변형되어 사용된다.\n",
    "- 다시 정리하면 아래와 같다.  \n",
    "  F(x) = 0.5 ∑^m (f_i(x))^2 = 0.5 ||f(x)||^2 = 0.5 f(x)^T*f(x) 이고  \n",
    "  F(x)가 최소인 x*의 값을 구하는 방법으로 비선형 최소자승법이라고 한다.  \n",
    "  ※ 여기서 m은 보통 샘플의 수를 나타낸다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Newton 탐색방법\n",
    "\n",
    "### 특징\n",
    "- Levenberg marquardt 탐색 방법의 기본방법이고, 충분히 작은 ||h||에 대하여  \n",
    "  l(h) = f(x + h) = f(x) + J(x)h 라는 근사식에서 시작한다.\n",
    "\n",
    "### 유도\n",
    "- F(x + h)의 근사식은 다음과 같다.  \n",
    "  F(x + h) = L(h) = 0.5 l(h)^T l(h)\n",
    "  = 0.5 * f^t(x) * f(x) + h^t * J^t(x) * f(x) + 0.5 * h^t * J^t(x) * J(x) * h  \n",
    "  = F(x) + h^t * J^t(x) * f(x) + 0.5 * h^t * J^t(x) * J(x) * h\n",
    "- L(h)가 최소가 되는 h를 찾으면,  \n",
    "  L'(h) = J^t(x) * f(x) + J^t(x) * J(x) * h  \n",
    "  L''(h) = J^t(x) * J(x) 이고\n",
    "- 최소값을 가지는 L'(h_gn) = 0에서  \n",
    "  J^t(x) * J(x) * h_gn = - J^t(x) * f(x)  \n",
    "  이다.\n",
    "- 만약 J(x)의 열벡터가 일차 독립이면, L''(h) = J^t(x)J(x)는 양의 확정행렬이고, 전역 최소값이 존재한다.\n",
    "- 여기서 구해진 h_gn 은 하강 방향벡터이자. L(h)의 유일한 전역 최소자이다.\n",
    "\n",
    "### 업데이트 방법\n",
    "1. J^t(x) * J(x) * h_gn = - J^t(x) * f(x)에서 h_gn을 구한다.\n",
    "2. x_{k+1} = x_k + a*h_gn 로 업데이트한다.  \n",
    "   ※ a는 step size로 직선탐색 알고리즘을 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 예제 1 가상의 a,b를 추론해보자. \n",
    "## y = a * cos(b*x) + b sin (a*x)\n",
    "\n",
    "sample_size = 100\n",
    "sample_x = 2* np.pi * np.random.rand(sample_size)\n",
    "noise = 0.1 * np.random.rand(sample_size)\n",
    "\n",
    "a = 100\n",
    "b = 102\n",
    "\n",
    "sample_y = a * np.cos( b * sample_x) + b * np.sin(a * sample_x)\n",
    "noised_sample_y = sample_y + noise\n",
    "iterations = 100\n",
    "\n",
    "def f(x, arg_a, arg_b):\n",
    "    return arg_a * np.cos( arg_b * x) + arg_b * np.sin(arg_a * x)\n",
    "\n",
    "def gradient_f(x, arg_a, arg_b):\n",
    "    gradient_a = np.cos( arg_b * x) + arg_b * np.cos(arg_a * x) * x\n",
    "    gradient_b = -1.0 * arg_a * np.sin( arg_b * x) * x + np.sin(arg_a * x)\n",
    "    return np.hstack((gradient_a, gradient_b))\n",
    "\n",
    "def F(x, y, arg_a, arg_b):\n",
    "    residual = y - f(x, arg_a, arg_b)\n",
    "    return (0.5 * residual.T() * residual)[0, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACPUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
